import logging
import warnings
from typing import Dict, List, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import plotly.express as px
import streamlit as st
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler, LabelEncoder

import os

# DeepSeek (OpenAI-compatible) SDK
try:
    from openai import OpenAI
    _HAS_OPENAI = True
except Exception:
    _HAS_OPENAI = False

DEEPSEEK_API_KEY_FALLBACK = "sk-ea05850f599f4e8686f23716f650f5b7"

# é…ç½®è­¦å‘Šå’Œæ—¥å¿—
warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False


# --- DeepSeek é›†æˆå·¥å…·å‡½æ•° ---
@st.cache_resource(show_spinner=False)
def get_deepseek_client():
    """
    æ„é€  DeepSeek å®¢æˆ·ç«¯ï¼š
    1) ä¼˜å…ˆè¯»å– st.secrets["DEEPSEEK_API_KEY"]ï¼›
    2) å…¶æ¬¡ç¯å¢ƒå˜é‡ DEEPSEEK_API_KEYï¼›
    3) æœ€åä½¿ç”¨æœ¬æ–‡ä»¶ä¸­çš„ DEEPSEEK_API_KEY_FALLBACKï¼ˆç”¨æˆ·æ˜¾å¼è¦æ±‚ï¼‰ã€‚
    """
    if not _HAS_OPENAI:
        return None
    api_key = None
    try:
        api_key = st.secrets.get("DEEPSEEK_API_KEY")
    except Exception:
        api_key = None
    if not api_key:
        api_key = os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        api_key = DEEPSEEK_API_KEY_FALLBACK
    if not api_key:
        return None
    try:
        # DeepSeek æä¾› OpenAI å…¼å®¹æ¥å£ï¼›ç›´æ¥æŒ‡å®š base_url å³å¯
        return OpenAI(api_key=api_key, base_url="https://api.deepseek.com")
    except Exception:
        return None


def _format_df_sample(df: pd.DataFrame, max_rows: int = 30, max_cols: int = 10) -> str:
    """å°†æ•°æ®æ ·æœ¬å‹ç¼©ä¸º CSV æ–‡æœ¬ï¼Œé¿å… token è¿‡è½½ã€‚"""
    if df is None or df.empty:
        return "(ç©ºæ•°æ®)"
    safe_cols = list(df.columns[:max_cols])
    sample = df.loc[:, safe_cols].head(max_rows)
    sample = sample.copy()
    for c in safe_cols:
        if sample[c].dtype == object:
            sample[c] = sample[c].astype(str).str.slice(0, 120)
    return sample.to_csv(index=False)


def _schema_summary(df: pd.DataFrame) -> str:
    if df is None or df.empty:
        return "(æ— )"
    dtypes = {c: str(t) for c, t in df.dtypes.items()}
    nunique = {c: int(df[c].nunique(dropna=True)) for c in df.columns}
    return pd.DataFrame({"dtype": dtypes, "nunique": nunique}).to_csv()


def call_deepseek(messages: list, model: str = "deepseek-chat") -> str:
    """è°ƒç”¨ DeepSeek èŠå¤©æ¥å£ï¼ˆOpenAI å…¼å®¹ï¼‰ã€‚"""
    client = get_deepseek_client()
    if client is None:
        return "[DeepSeek æœªé…ç½®] è¯·åœ¨ Streamlit secrets æˆ–ç¯å¢ƒå˜é‡ä¸­è®¾ç½® DEEPSEEK_API_KEYï¼Œå†é‡è¯•ã€‚"
    try:
        resp = client.chat.completions.create(
            model=model,
            messages=messages,
        )
        return resp.choices[0].message.content or "(æ— è¿”å›å†…å®¹)"
    except Exception as e:
        return f"[DeepSeek è°ƒç”¨å¤±è´¥] {e}"


def call_deepseek_stream(messages: list, model: str = "deepseek-chat"):
    """DeepSeek æµå¼è¾“å‡ºï¼ˆOpenAI å…¼å®¹ï¼‰ï¼šé€å— yield æ–‡æœ¬ã€‚"""
    client = get_deepseek_client()
    if client is None:
        yield "[DeepSeek æœªé…ç½®] è¯·åœ¨ Streamlit secrets æˆ–ç¯å¢ƒå˜é‡ä¸­è®¾ç½® DEEPSEEK_API_KEYã€‚"
        return
    try:
        stream = client.chat.completions.create(
            model=model,
            messages=messages,
            stream=True
        )
        for chunk in stream:
            delta = None
            try:
                delta = chunk.choices[0].delta.content
            except Exception:
                pass
            if delta:
                yield delta
    except Exception as e:
        yield f"\n[DeepSeek æµå¼è°ƒç”¨å¤±è´¥] {e}"


def build_audit_prompt_header() -> str:
    return (
        "ä½ æ˜¯èµ„æ·±å®¡è®¡ä¸é£æ§åˆ†æä¸“å®¶ï¼Œè¯·åŸºäºç»™å®šçš„æ•°æ®ç‰‡æ®µä¸ç»Ÿè®¡ç»“æœï¼Œç”¨ç®€æ˜ä¸­æ–‡ç»™å‡ºï¼š"
        "(1) å…³é”®å‘ç°ï¼›(2) åˆç†æ€§/å¼‚å¸¸æ€§çš„è¯æ®ï¼›(3) å»ºè®®çš„æ ¸æŸ¥åŠ¨ä½œï¼›(4) å¯å¤åˆ¶çš„è§„åˆ™/é˜ˆå€¼ã€‚"
        " è¾“å‡ºé‡‡ç”¨ Markdown åˆ†èŠ‚ä¸åˆ—è¡¨ï¼Œé¿å…æ³„éœ²ä¸ªäººéšç§ï¼Œè°¨æ…ä¸‹ç»“è®ºã€‚"
    )

# --- é…ç½®å’Œå¸¸é‡ ---
class Config:
    """é…ç½®ç±»"""
    PAGE_TITLE = "æ™ºèƒ½å®¡è®¡AIå¹³å°"
    APP_NAME = "æ™ºé‰´AuditGPT"
    VERSION = "v2.0"
    MAX_FILE_SIZE = 200  # MB
    SUPPORTED_FORMATS = ['csv', 'xlsx', 'xls']
    DEFAULT_CONTAMINATION = 0.02
    RANDOM_STATE = 42


# --- é¡µé¢é…ç½® ---
st.set_page_config(
    page_title=Config.PAGE_TITLE,
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- è‡ªå®šä¹‰CSSæ ·å¼ ---
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 2rem;
        border-radius: 10px;
        margin-bottom: 2rem;
        text-align: center;
    }
    .metric-card {
        background: white;
        padding: 1rem;
        border-radius: 10px;
        border-left: 4px solid #667eea;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        margin: 1rem 0;
    }
    .anomaly-card {
        background: #ffebee;
        border-left: 4px solid #f44336;
        padding: 1rem;
        border-radius: 5px;
        margin: 1rem 0;
    }
    .normal-card {
        background: #e8f5e8;
        border-left: 4px solid #4caf50;
        padding: 1rem;
        border-radius: 5px;
        margin: 1rem 0;
    }
    .stButton > button {
        width: 100%;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        padding: 0.5rem 2rem;
        border-radius: 25px;
        font-weight: bold;
        transition: all 0.3s;
    }
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 8px rgba(102, 126, 234, 0.3);
    }
    .risk-high {
        background: linear-gradient(135deg, #ff6b6b 0%, #c0392b 100%);
        color: white;
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
    }
    .risk-medium {
        background: linear-gradient(135deg, #ffeaa7 0%, #fdcb6e 100%);
        color: #2d3436;
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
    }
    .risk-low {
        background: linear-gradient(135deg, #55efc4 0%, #00b894 100%);
        color: white;
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
    }
</style>
""", unsafe_allow_html=True)

# --- æ ‡é¢˜éƒ¨åˆ† ---
st.markdown(f"""
<div class="main-header">
    <h1>ğŸ” {Config.APP_NAME}</h1>
    <p>åŸºäºæœºå™¨å­¦ä¹ çš„å¯è§£é‡Šæ€§æ™ºèƒ½å®¡è®¡å¼‚å¸¸æ£€æµ‹å¹³å° {Config.VERSION}</p>
</div>
""", unsafe_allow_html=True)


# --- ä¼šè¯çŠ¶æ€åˆå§‹åŒ– ---
def initialize_session_state():
    """åˆå§‹åŒ–ä¼šè¯çŠ¶æ€"""
    default_states = {
        'df': None,
        'processed_df': None,
        'anomalies_df': None,
        'model': None,
        'scaler': None,
        'X_scaled': None,
        'label_encoders': {},
        'feature_names': [],
        'analysis_complete': False,
        'file_name': None,
        'deepseek_model': 'deepseek-chat',
        'detailed_report_visible': False,
        'last_report_lines': [],
        'last_feature_impacts': [],
        'last_selected_index': None,
        'last_selected_anomaly_score': None,
        'last_impact_df_records': [],
        'ds_expander_open': False,
        'ds_result_text': ""
    }

    for key, value in default_states.items():
        if key not in st.session_state:
            st.session_state[key] = value


initialize_session_state()


# --- æ•°æ®å¤„ç†å‡½æ•° ---
class DataProcessor:
    """æ•°æ®å¤„ç†ç±»"""

    @staticmethod
    def detect_data_types(df: pd.DataFrame) -> Dict[str, List[str]]:
        """è‡ªåŠ¨æ£€æµ‹æ•°æ®ç±»å‹"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
        datetime_cols = []

        # å°è¯•æ£€æµ‹æ—¥æœŸæ—¶é—´åˆ—
        for col in categorical_cols:
            sample = df[col].dropna().head(100)
            try:
                pd.to_datetime(sample)
                datetime_cols.append(col)
            except:
                pass

        # ä»åˆ†ç±»åˆ—ä¸­ç§»é™¤æ—¥æœŸæ—¶é—´åˆ—
        categorical_cols = [col for col in categorical_cols if col not in datetime_cols]

        return {
            'numeric': numeric_cols,
            'categorical': categorical_cols,
            'datetime': datetime_cols
        }

    @staticmethod
    def preprocess_data(df: pd.DataFrame, target_features: List[str] = None) -> Tuple[pd.DataFrame, Dict]:
        """æ•°æ®é¢„å¤„ç†"""
        processed_df = df.copy()
        encoders = {}

        # è‡ªåŠ¨æ£€æµ‹ç‰¹å¾ç±»å‹
        data_types = DataProcessor.detect_data_types(processed_df)

        # å¦‚æœæ²¡æœ‰æŒ‡å®šç›®æ ‡ç‰¹å¾ï¼Œè‡ªåŠ¨é€‰æ‹©æ•°å€¼å‹ç‰¹å¾
        if target_features is None:
            target_features = data_types['numeric']

        # å¤„ç†åˆ†ç±»å˜é‡
        for col in data_types['categorical']:
            if col in target_features:
                le = LabelEncoder()
                processed_df[col] = le.fit_transform(processed_df[col].astype(str))
                encoders[col] = le

        # å¤„ç†æ—¥æœŸæ—¶é—´å˜é‡
        for col in data_types['datetime']:
            if col in target_features:
                processed_df[col] = pd.to_datetime(processed_df[col])
                # æå–æœ‰ç”¨çš„æ—¶é—´ç‰¹å¾
                processed_df[f'{col}_hour'] = processed_df[col].dt.hour
                processed_df[f'{col}_dayofweek'] = processed_df[col].dt.dayofweek
                processed_df[f'{col}_month'] = processed_df[col].dt.month

                # æ›´æ–°ç›®æ ‡ç‰¹å¾åˆ—è¡¨
                target_features.extend([f'{col}_hour', f'{col}_dayofweek', f'{col}_month'])
                if col in target_features:
                    target_features.remove(col)

        # å¤„ç†ç¼ºå¤±å€¼
        processed_df = processed_df.fillna(processed_df.mean(numeric_only=True))
        processed_df = processed_df.fillna('Unknown')

        return processed_df, encoders


class ModelAnalyzer:
    """æ¨¡å‹åˆ†æç±»"""

    @staticmethod
    def train_isolation_forest(X: pd.DataFrame, contamination: float = Config.DEFAULT_CONTAMINATION) -> Tuple[
        IsolationForest, StandardScaler]:
        """è®­ç»ƒIsolation Forestæ¨¡å‹"""
        # æ•°æ®æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # è®­ç»ƒæ¨¡å‹
        model = IsolationForest(
            contamination=contamination,
            random_state=Config.RANDOM_STATE,
            n_estimators=100,
            max_samples='auto',
            n_jobs=-1
        )
        model.fit(X_scaled)

        return model, scaler

    @staticmethod
    def generate_advanced_report(anomaly_score: float, feature_names: List[str],
                                 sample_data: pd.Series) -> Tuple[List[str], List[Dict]]:
        """ç”Ÿæˆé«˜çº§è§£é‡ŠæŠ¥å‘Šï¼ˆç®€åŒ–ç‰ˆï¼Œä¸ä½¿ç”¨SHAPï¼‰"""
        report = []

        # å¼‚å¸¸å¾—åˆ†è§£é‡Š
        risk_level = "é«˜é£é™©" if anomaly_score < -0.5 else "ä¸­é£é™©" if anomaly_score < -0.2 else "ä½é£é™©"
        risk_color = "ğŸ”´" if risk_level == "é«˜é£é™©" else "ğŸŸ¡" if risk_level == "ä¸­é£é™©" else "ğŸŸ¢"

        report.append("### 1. å¼‚å¸¸é£é™©è¯„ä¼°")
        report.append(f"{risk_color} **é£é™©ç­‰çº§**: {risk_level}")
        report.append(f"**å¼‚å¸¸å¾—åˆ†**: {anomaly_score:.4f} (è¶Šä½è¶Šå¼‚å¸¸)")
        report.append("")

        # ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
        feature_impacts = []

        # è¿™é‡Œä½¿ç”¨ç®€å•çš„ç‰¹å¾å€¼åˆ†æä½œä¸ºæ›¿ä»£
        for feature in feature_names:
            if feature in sample_data.index:
                value = sample_data[feature]
                # ç®€å•çš„å¯å‘å¼è§„åˆ™ï¼šæç«¯å€¼å¯èƒ½æ›´é‡è¦
                if isinstance(value, (int, float)):
                    # å‡è®¾æ•°å€¼å‹ç‰¹å¾çš„æç«¯å€¼æ›´å¯èƒ½å¼‚å¸¸
                    impact = abs(value - sample_data[feature].mean()) / sample_data[feature].std() if sample_data[
                                                                                                          feature].std() > 0 else 0
                    direction = "æ¨å¼‚å¸¸" if abs(value) > 2 else "æ¨æ­£å¸¸"  # å‡è®¾ç»å¯¹å€¼å¤§äº2æ ‡å‡†å·®ä¸ºå¼‚å¸¸
                    color = "ğŸ”´" if direction == "æ¨å¼‚å¸¸" else "ğŸ”µ"

                    feature_impacts.append({
                        'ç‰¹å¾åç§°': feature,
                        'ç‰¹å¾å€¼': round(value, 4),
                        'å½±å“æ–¹å‘': direction,
                        'å½±å“ç¨‹åº¦': "é«˜" if impact > 2 else "ä¸­" if impact > 1 else "ä½",
                        'SHAPå€¼': round(impact, 4),
                        'é‡è¦æ€§æ’å': 0,
                        'æ ‡å¿—': color
                    })

        # æŒ‰å½±å“å€¼æ’åºå¹¶åˆ†é…æ’å
        feature_impacts.sort(key=lambda x: x['SHAPå€¼'], reverse=True)
        for i, impact in enumerate(feature_impacts):
            impact['é‡è¦æ€§æ’å'] = i + 1

        # ç”Ÿæˆå…³é”®å‘ç°
        report.append("### 2. å…³é”®å‘ç°")

        anomaly_drivers = [f for f in feature_impacts if f['å½±å“æ–¹å‘'] == 'æ¨å¼‚å¸¸'][:3]
        normal_drivers = [f for f in feature_impacts if f['å½±å“æ–¹å‘'] == 'æ¨æ­£å¸¸'][:3]

        if anomaly_drivers:
            report.append("**âŒ ä¸»è¦å¼‚å¸¸é©±åŠ¨å› ç´ ï¼š**")
            for i, driver in enumerate(anomaly_drivers, 1):
                report.append(f"{i}. **{driver['ç‰¹å¾åç§°']}** = {driver['ç‰¹å¾å€¼']}")
                report.append(f"   - å½±å“æ–¹å‘: {driver['æ ‡å¿—']} {driver['å½±å“æ–¹å‘']}")
                report.append(f"   - å½±å“ç¨‹åº¦: {driver['å½±å“ç¨‹åº¦']}")
            report.append("")

        if normal_drivers:
            report.append("**âœ… ä¸»è¦æ­£å¸¸é©±åŠ¨å› ç´ ï¼š**")
            for i, driver in enumerate(normal_drivers, 1):
                report.append(f"{i}. **{driver['ç‰¹å¾åç§°']}** = {driver['ç‰¹å¾å€¼']}")
                report.append(f"   - å½±å“æ–¹å‘: {driver['æ ‡å¿—']} {driver['å½±å“æ–¹å‘']}")
                report.append(f"   - å½±å“ç¨‹åº¦: {driver['å½±å“ç¨‹åº¦']}")
            report.append("")

        # å®¡è®¡å»ºè®®
        report.append("### 3. æ™ºèƒ½å®¡è®¡å»ºè®®")
        if risk_level == "é«˜é£é™©":
            report.append("ğŸš¨ **ç´§æ€¥å…³æ³¨**")
            report.append("1. ç«‹å³è¿›è¡Œè¯¦ç»†å®¡æŸ¥")
            report.append("2. æ ¸å®æ‰€æœ‰ç›¸å…³å•æ®å’Œå‡­è¯")
            report.append("3. è”ç³»ç›¸å…³ä¸šåŠ¡äººå‘˜ç¡®è®¤")
            if anomaly_drivers:
                report.append(f"4. é‡ç‚¹æ£€æŸ¥ **{anomaly_drivers[0]['ç‰¹å¾åç§°']}** ç›¸å…³ä¸šåŠ¡")
        elif risk_level == "ä¸­é£é™©":
            report.append("âš ï¸ **é‡ç‚¹å…³æ³¨**")
            report.append("1. è¿›è¡ŒæŠ½æ ·å¤æ ¸")
            report.append("2. ä¸å†å²æ•°æ®è¿›è¡Œå¯¹æ¯”")
            report.append("3. å¿…è¦æ—¶è¿›è¡Œè¿›ä¸€æ­¥è°ƒæŸ¥")
        else:
            report.append("â„¹ï¸ **å¸¸è§„å¤„ç†**")
            report.append("1. æŒ‰å¸¸è§„å®¡è®¡ç¨‹åºå¤„ç†")
            report.append("2. å¯ä½œä¸ºå¯¹æ¯”åŸºå‡†")

        return report, feature_impacts


# --- ä¾§è¾¹æ é…ç½® ---
with st.sidebar:
    st.markdown("### âš™ï¸ æ¨¡å‹å‚æ•°é…ç½®")

    contamination = st.slider(
        "å¼‚å¸¸æ¯”ä¾‹ (%)",
        min_value=0.1,
        max_value=10.0,
        value=2.0,
        step=0.1,
        help="é¢„æœŸæ•°æ®ä¸­å¼‚å¸¸æ ·æœ¬çš„æ¯”ä¾‹"
    ) / 100

    st.markdown("### ğŸ“ˆ åˆ†æé€‰é¡¹")
    show_data_profile = st.checkbox("æ˜¾ç¤ºæ•°æ®æ¦‚è§ˆ", value=True)
    show_correlation = st.checkbox("æ˜¾ç¤ºç‰¹å¾ç›¸å…³æ€§", value=True)
    auto_feature_selection = st.checkbox("è‡ªåŠ¨ç‰¹å¾é€‰æ‹©", value=True)

# --- ä¸»è¦å†…å®¹åŒºåŸŸ ---
st.header(" 1. æ•°æ®ä¸Šä¼ ä¸é¢„è§ˆ")

# æ–‡ä»¶ä¸Šä¼ 
uploaded_file = st.file_uploader(
    "è¯·ä¸Šä¼ ç»è¿‡é¢„å¤„ç†åçš„å®¡è®¡æ•°æ®æ–‡ä»¶",
    type=Config.SUPPORTED_FORMATS,
    help=f"æ”¯æŒæ ¼å¼: {', '.join(Config.SUPPORTED_FORMATS)}ï¼Œæœ€å¤§æ–‡ä»¶å¤§å°: {Config.MAX_FILE_SIZE}MB"
)

if uploaded_file is not None:
    try:
        # è¯»å–æ–‡ä»¶
        if st.session_state.file_name != uploaded_file.name:
            with st.spinner('æ­£åœ¨è¯»å–æ–‡ä»¶...'):
                if uploaded_file.name.endswith('.csv'):
                    df = pd.read_csv(uploaded_file)
                else:
                    df = pd.read_excel(uploaded_file)

                st.session_state.df = df
                st.session_state.file_name = uploaded_file.name
                st.session_state.analysis_complete = False

            st.success(f"âœ… æ–‡ä»¶ '{uploaded_file.name}' ä¸Šä¼ æˆåŠŸï¼")

        df = st.session_state.df

        # æ•°æ®åŸºæœ¬ä¿¡æ¯
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("æ€»è®°å½•æ•°", len(df))
        with col2:
            st.metric("ç‰¹å¾æ•°é‡", len(df.columns))
        with col3:
            st.metric("ç¼ºå¤±å€¼", df.isnull().sum().sum())
        with col4:
            st.metric("é‡å¤è¡Œ", df.duplicated().sum())

        # æ•°æ®é¢„è§ˆ
        st.subheader(" æ•°æ®é¢„è§ˆ")
        preview_rows = st.selectbox("æ˜¾ç¤ºè¡Œæ•°", [5, 10, 20, 50], index=0)
        st.dataframe(df.head(preview_rows), use_container_width=True)

        # æ•°æ®æ¦‚è§ˆ
        if show_data_profile:
            with st.expander(" æ•°æ®æ¦‚è§ˆåˆ†æ", expanded=False):
                col1, col2 = st.columns(2)

                with col1:
                    st.subheader("æ•°æ®ç±»å‹åˆ†å¸ƒ")
                    data_types = DataProcessor.detect_data_types(df)
                    type_counts = {k: len(v) for k, v in data_types.items()}
                    color_palette = px.colors.qualitative.Safe

                    fig = px.pie(
                        values=list(type_counts.values()),
                        names=list(type_counts.keys()),
                        title="ç‰¹å¾ç±»å‹åˆ†å¸ƒ",
                        color_discrete_sequence=color_palette,
                        hole=0.4
                    )
                    fig.update_traces(
                        textposition='inside',
                        textinfo='percent+label',
                        insidetextfont=dict(size=12, color='white'),
                        outsidetextfont=dict(size=10),
                        marker=dict(line=dict(color='white', width=2))
                    )
                    fig.update_layout(
                        showlegend=True,
                        legend=dict(
                            orientation="v",
                            yanchor="top",
                            y=1,
                            xanchor="left",
                            x=1.1
                        )
                    )
                    st.plotly_chart(fig, use_container_width=True)

                with col2:
                    st.subheader("ç¼ºå¤±å€¼ç»Ÿè®¡")
                    missing_data = df.isnull().sum()
                    missing_data = missing_data[missing_data > 0]

                    if not missing_data.empty:
                        fig = px.bar(
                            x=missing_data.index,
                            y=missing_data.values,
                            title="å„ç‰¹å¾ç¼ºå¤±å€¼æ•°é‡",
                            color=missing_data.values,
                            color_continuous_scale='Reds'
                        )
                        fig.update_xaxis(tickangle=45)
                        st.plotly_chart(fig, use_container_width=True)
                    else:
                        st.info("âœ… æ•°æ®å®Œæ•´ï¼Œæ— ç¼ºå¤±å€¼ï¼")

        # ç‰¹å¾é€‰æ‹©
        st.markdown("---")
        st.header(" 2. ç‰¹å¾é€‰æ‹©ä¸é…ç½®")

        if auto_feature_selection:
            # è‡ªåŠ¨ç‰¹å¾é€‰æ‹©
            data_types = DataProcessor.detect_data_types(df)
            suggested_features = data_types['numeric']

            # æ·»åŠ ä¸€äº›å¯èƒ½çš„åˆ†ç±»ç‰¹å¾
            categorical_features = [col for col in data_types['categorical']
                                    if df[col].nunique() < 20]  # é™åˆ¶åˆ†ç±»æ•°é‡
            suggested_features.extend(categorical_features[:3])  # æœ€å¤šæ·»åŠ 3ä¸ªåˆ†ç±»ç‰¹å¾

            st.info(f" è‡ªåŠ¨æ¨èç‰¹å¾: {', '.join(suggested_features)}")
        else:
            suggested_features = []

        # æ‰‹åŠ¨ç‰¹å¾é€‰æ‹©
        available_columns = df.columns.tolist()
        selected_features = st.multiselect(
            "é€‰æ‹©ç”¨äºå¼‚å¸¸æ£€æµ‹çš„ç‰¹å¾",
            available_columns,
            default=suggested_features,
            help="é€‰æ‹©æ•°å€¼å‹ç‰¹å¾æ•ˆæœæ›´å¥½ï¼Œåˆ†ç±»ç‰¹å¾ä¼šè‡ªåŠ¨ç¼–ç "
        )

        if len(selected_features) < 2:
            st.warning("âš ï¸ è¯·è‡³å°‘é€‰æ‹©2ä¸ªç‰¹å¾è¿›è¡Œå¼‚å¸¸æ£€æµ‹")
        else:
            # ç‰¹å¾ç›¸å…³æ€§åˆ†æ
            if show_correlation and len(selected_features) > 2:
                with st.expander(" ç‰¹å¾ç›¸å…³æ€§åˆ†æ", expanded=False):
                    numeric_features = df[selected_features].select_dtypes(include=[np.number]).columns
                    if len(numeric_features) > 1:
                        corr_matrix = df[numeric_features].corr()

                        fig = px.imshow(
                            corr_matrix,
                            title="ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾",
                            color_continuous_scale = "Blues" ,
                            aspect="auto"
                        )
                        st.plotly_chart(fig, use_container_width=True)

                        # é«˜ç›¸å…³æ€§æé†’
                        high_corr = np.where(np.abs(corr_matrix) > 0.8)
                        high_corr_pairs = [(corr_matrix.index[x], corr_matrix.columns[y])
                                           for x, y in zip(*high_corr) if x != y]
                        if high_corr_pairs:
                            st.warning(f"âš ï¸ å‘ç°é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹: {high_corr_pairs[:3]}")

            # æ¨¡å‹è®­ç»ƒ
            st.markdown("---")
            st.header(" 3. å¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒ")

            if st.button(" å¼€å§‹æ™ºèƒ½åˆ†æ", type="primary"):
                with st.spinner('ğŸ”„ AIæ¨¡å‹æ­£åœ¨è¿›è¡Œå¼‚å¸¸æ£€æµ‹åˆ†æ...'):
                    try:
                        # æ•°æ®é¢„å¤„ç†
                        processed_df, encoders = DataProcessor.preprocess_data(df, selected_features)

                        # å‡†å¤‡ç‰¹å¾çŸ©é˜µ
                        feature_cols = []
                        for feature in selected_features:
                            if feature in processed_df.columns:
                                feature_cols.append(feature)
                            else:
                                # æŸ¥æ‰¾å¯èƒ½çš„æ´¾ç”Ÿç‰¹å¾ï¼ˆå¦‚æ—¶é—´ç‰¹å¾ï¼‰
                                derived = [col for col in processed_df.columns if col.startswith(f'{feature}_')]
                                feature_cols.extend(derived)

                        X = processed_df[feature_cols]

                        # è®­ç»ƒæ¨¡å‹
                        model, scaler = ModelAnalyzer.train_isolation_forest(X, contamination)

                        # é¢„æµ‹
                        X_scaled = scaler.transform(X)
                        predictions = model.predict(X_scaled)
                        anomaly_scores = model.decision_function(X_scaled)

                        # ä¿å­˜ç»“æœ
                        processed_df['å¼‚å¸¸æ ‡è¯†'] = predictions
                        processed_df['å¼‚å¸¸å¾—åˆ†'] = anomaly_scores
                        processed_df['å¼‚å¸¸åˆ¤å®š'] = processed_df['å¼‚å¸¸æ ‡è¯†'].apply(
                            lambda x: "å¼‚å¸¸" if x == -1 else "æ­£å¸¸")

                        # æ›´æ–°ä¼šè¯çŠ¶æ€
                        st.session_state.processed_df = processed_df
                        st.session_state.anomalies_df = processed_df[processed_df['å¼‚å¸¸åˆ¤å®š'] == 'å¼‚å¸¸'].copy()
                        st.session_state.model = model
                        st.session_state.scaler = scaler
                        st.session_state.X_scaled = X_scaled
                        st.session_state.label_encoders = encoders
                        st.session_state.feature_names = feature_cols
                        st.session_state.analysis_complete = True

                        st.success("âœ… åˆ†æå®Œæˆï¼")
                        st.rerun()

                    except Exception as e:
                        st.error(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
                        st.info("è¯·æ£€æŸ¥æ•°æ®æ ¼å¼æˆ–è”ç³»æŠ€æœ¯æ”¯æŒ")

    except Exception as e:
        st.error(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}")

# --- ç»“æœå±•ç¤º ---
if st.session_state.analysis_complete and st.session_state.anomalies_df is not None:
    st.markdown("---")
    st.header(" 4. å¼‚å¸¸æ£€æµ‹ç»“æœ")

    anomalies_df = st.session_state.anomalies_df
    total_records = len(st.session_state.processed_df)
    anomaly_count = len(anomalies_df)
    anomaly_rate = (anomaly_count / total_records) * 100

    # ç»“æœç»Ÿè®¡
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("æ€»è®°å½•æ•°", total_records)
    with col2:
        st.metric("å¼‚å¸¸è®°å½•æ•°", anomaly_count, delta=f"{anomaly_count}")
    with col3:
        st.metric("å¼‚å¸¸æ¯”ä¾‹", f"{anomaly_rate:.2f}%")
    with col4:
        avg_score = anomalies_df['å¼‚å¸¸å¾—åˆ†'].mean()
        st.metric("å¹³å‡å¼‚å¸¸å¾—åˆ†", f"{avg_score:.3f}")

    # å¼‚å¸¸è®°å½•å±•ç¤º
    st.subheader(" å¼‚å¸¸äº¤æ˜“åˆ—è¡¨")
    st.dataframe(anomalies_df.head(20), use_container_width=True)

    if len(anomalies_df) > 20:
        st.info(f"å…±å‘ç° {len(anomalies_df)} æ¡å¼‚å¸¸è®°å½•ï¼Œä»…æ˜¾ç¤ºå‰20æ¡")

    # å¼‚å¸¸å¾—åˆ†åˆ†å¸ƒ
    st.subheader(" å¼‚å¸¸å¾—åˆ†åˆ†å¸ƒ")
    fig = px.histogram(
        st.session_state.processed_df,
        x='å¼‚å¸¸å¾—åˆ†',
        color='å¼‚å¸¸åˆ¤å®š',
        title='å¼‚å¸¸å¾—åˆ†åˆ†å¸ƒå›¾',
        nbins=50,
        color_discrete_map={'å¼‚å¸¸': 'red', 'æ­£å¸¸': 'green'}
    )
    fig.update_layout(bargap=0.1)
    st.plotly_chart(fig, use_container_width=True)

    # è¯¦ç»†åˆ†æ
    st.markdown("---")
    st.header(" 5. è¯¦ç»†åˆ†æ")

    # é€‰æ‹©è¦åˆ†æçš„å¼‚å¸¸è®°å½•
    if not anomalies_df.empty:
        selected_index = st.selectbox(
            "é€‰æ‹©è¦è¯¦ç»†åˆ†æçš„å¼‚å¸¸äº¤æ˜“",
            anomalies_df.index,
            format_func=lambda x: f"è®°å½• {x} (å¾—åˆ†: {anomalies_df.loc[x, 'å¼‚å¸¸å¾—åˆ†']:.3f})"
        )

        if selected_index is not None:
            selected_anomaly = anomalies_df.loc[selected_index]
            st.subheader(f" å¼‚å¸¸äº¤æ˜“è¯¦æƒ… - è®°å½• {selected_index}")

            # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("**äº¤æ˜“ç‰¹å¾å€¼:**")
                for feature in st.session_state.feature_names:
                    if feature in selected_anomaly:
                        st.write(f"- {feature}: {selected_anomaly[feature]}")
            with col2:
                st.markdown("**å¼‚å¸¸ä¿¡æ¯:**")
                st.write(f"- å¼‚å¸¸å¾—åˆ†: {selected_anomaly['å¼‚å¸¸å¾—åˆ†']:.4f}")
                st.write(f"- å¼‚å¸¸æ ‡è¯†: {selected_anomaly['å¼‚å¸¸åˆ¤å®š']}")

            # ç”Ÿæˆè§£é‡ŠæŠ¥å‘Šï¼ˆæŒä¹…åŒ–åˆ° session_stateï¼Œé¿å…æŒ‰é’®è§¦å‘é‡è·‘åæŠ˜å ï¼‰
            generate_clicked = st.button(" ç”Ÿæˆè¯¦ç»†è§£é‡ŠæŠ¥å‘Š", key="btn_generate_report")

            # æ¯æ¬¡é€‰æ‹©è®°å½•æ—¶ï¼Œä¿å­˜å½“å‰ç´¢å¼•ä¸å¾—åˆ†ï¼ˆç”¨äº DeepSeek ä¸Šä¸‹æ–‡ä¸æŒä¹…æ¸²æŸ“ï¼‰
            st.session_state['last_selected_index'] = int(selected_index)
            st.session_state['last_selected_anomaly_score'] = float(selected_anomaly['å¼‚å¸¸å¾—åˆ†'])

            if generate_clicked:
                with st.spinner('æ­£åœ¨ç”Ÿæˆè§£é‡ŠæŠ¥å‘Š...'):
                    report, feature_impacts = ModelAnalyzer.generate_advanced_report(
                        selected_anomaly['å¼‚å¸¸å¾—åˆ†'],
                        st.session_state.feature_names,
                        selected_anomaly
                    )
                    # æŒä¹…åŒ–
                    st.session_state['last_report_lines'] = report
                    st.session_state['last_feature_impacts'] = feature_impacts
                    st.session_state['last_impact_df_records'] = feature_impacts  # ä»¥ records å½¢å¼å­˜å‚¨
                    st.session_state['detailed_report_visible'] = True
                    # æ‰“å¼€ DeepSeek é¢æ¿ï¼ˆç”¨æˆ·é€šå¸¸ä¼šç»§ç»­ä½¿ç”¨ï¼‰
                    st.session_state['ds_expander_open'] = True

            # æ¸²æŸ“ï¼šå¦‚æœç‚¹å‡»è¿‡æŒ‰é’®ï¼Œæˆ–å·²æœ‰æŒä¹…åŒ–å†…å®¹ï¼Œåˆ™æ˜¾ç¤ºè¯¦ç»†æŠ¥å‘Š
            if st.session_state.get('detailed_report_visible'):
                report = st.session_state.get('last_report_lines', [])
                feature_impacts = st.session_state.get('last_feature_impacts', [])

                st.markdown("---")
                st.header("ğŸ“‹ å¼‚å¸¸äº¤æ˜“è§£é‡ŠæŠ¥å‘Š")
                report_col, chart_col = st.columns([2, 1])
                with report_col:
                    for line in report:
                        st.markdown(line)
                with chart_col:
                    st.subheader(" é£é™©é›·è¾¾å›¾")
                    risk_categories = ['é‡‘é¢é£é™©', 'æ—¶é—´é£é™©', 'é¢‘ç‡é£é™©', 'ç±»å‹é£é™©']
                    risk_scores = [
                        min(100, max(0, (abs(selected_anomaly.get('äº¤æ˜“é‡‘é¢', 0) - 1000) / 1000) * 100)),
                        min(100, max(0, (abs(selected_anomaly.get('äº¤æ˜“æ—¶é—´é—´éš”', 0) - 5) / 5) * 100)),
                        min(100, max(0, (abs(selected_anomaly.get('è´¦æˆ·å†å²äº¤æ˜“ç¬”æ•°', 0) - 50) / 50) * 100)),
                        min(100, max(0, selected_anomaly.get('å¼‚å¸¸å¾—åˆ†', 0) * -25))
                    ]
                    fig_radar = px.line_polar(
                        r=risk_scores + [risk_scores[0]],
                        theta=risk_categories + [risk_categories[0]],
                        line_close=True,
                        range_r=[0, 100],
                        title="å¤šç»´åº¦é£é™©è¯„ä¼°"
                    )
                    fig_radar.update_traces(fill='toself', line_color='#ff6b6b')
                    st.plotly_chart(fig_radar, use_container_width=True)

                # ç‰¹å¾å½±å“è¡¨æ ¼
                st.subheader("4. ç‰¹å¾å½±å“æ±‡æ€»è¡¨")
                impact_df = pd.DataFrame(st.session_state.get('last_impact_df_records', []))
                if not impact_df.empty:
                    impact_df['é¢œè‰²æ ‡è¯†'] = impact_df['å½±å“æ–¹å‘'].map({'æ¨å¼‚å¸¸': 'ğŸ”´', 'æ¨æ­£å¸¸': 'ğŸ”µ'})
                    st.dataframe(
                        impact_df[['é¢œè‰²æ ‡è¯†', 'ç‰¹å¾åç§°', 'ç‰¹å¾å€¼', 'å½±å“æ–¹å‘', 'å½±å“ç¨‹åº¦', 'SHAPå€¼', 'é‡è¦æ€§æ’å']],
                        use_container_width=True,
                        height=400
                    )
                    csv = impact_df.to_csv(index=False).encode('utf-8')
                    st.download_button(
                        label="â¬‡ï¸ ä¸‹è½½ç‰¹å¾å½±å“åˆ†æè¡¨",
                        data=csv,
                        file_name=f'anomaly_analysis_{st.session_state.get("last_selected_index")}.csv',
                        mime='text/csv'
                    )
                else:
                    st.info("æ— æ³•ç”Ÿæˆè¯¦ç»†çš„ç‰¹å¾å½±å“åˆ†æ")

                # â€”â€”â€” DeepSeekï¼šæ¶¦è‰²ä¸æ‰©å±•è§£é‡Šï¼ˆå¯é€‰ï¼‰ â€”â€”â€”
                with st.expander(
                    "ğŸ§  ç”¨ DeepSeek å¯¹ä¸Šè¿°æŠ¥å‘Šè¿›è¡ŒAIæ–‡å­—å¤„ç†ï¼ˆæ¶¦è‰²/æ‰©å±•/ç”Ÿæˆå¯æ‰§è¡Œå»ºè®®ï¼‰",
                    expanded=st.session_state.get('ds_expander_open', False)
                ):
                    if not _HAS_OPENAI:
                        st.warning("æœªæ£€æµ‹åˆ° openai SDKã€‚è¯·å…ˆæ‰§è¡Œ `pip install openai` åé‡å¯åº”ç”¨ã€‚")
                    model_choice = st.selectbox(
                        "é€‰æ‹©DeepSeekæ¨¡å‹",
                        options=["deepseek-chat", "deepseek-reasoner"],
                        index=0,
                        help="deepseek-chatï¼šå¿«é€Ÿç›´è¿”ï¼›deepseek-reasonerï¼šå¸¦æ¨ç†çš„æ›´è¯¦å°½å›ç­”ã€‚"
                    )
                    stream_on = st.checkbox("å®æ—¶æ˜¾ç¤ºï¼ˆæµå¼ï¼‰", value=True, help="å‹¾é€‰åå°†è¾¹ç”Ÿæˆè¾¹æ˜¾ç¤ºã€‚", key="cb_stream_explain")
                    custom_instruction = st.text_area(
                        "å¯é€‰ï¼šæ·»åŠ ä½ çš„é¢å¤–æŒ‡ä»¤ï¼ˆä¾‹å¦‚ï¼šç”Ÿæˆé¢å‘é¢†å¯¼çš„æ‘˜è¦ã€ç”Ÿæˆè‹±æ–‡æ‘˜è¦ã€è¾“å‡º JSON è§„åˆ™ç­‰ï¼‰",
                        value="è¯·æŠŠä¸Šé¢çš„æŠ¥å‘Šé‡å†™ä¸ºé«˜ç®¡æ‘˜è¦ï¼Œå¹¶è¿½åŠ 3æ¡å¯æ‰§è¡Œæ ¸æŸ¥æ¸…å•ä¸å¯å¤åˆ¶çš„é˜ˆå€¼å»ºè®®ã€‚",
                        height=100
                    )
                    if st.button("ğŸš€ è°ƒç”¨ DeepSeek ç”Ÿæˆæ–‡å­—", key="btn_ds_explain"):
                        st.session_state['ds_expander_open'] = True  # ä¿æŒå±•å¼€
                        # ä»ä¼šè¯ä¸­å–æŒä¹…åŒ–çš„è¡¨æ ¼ä¸è¦ç‚¹
                        df_scope = st.session_state.get('processed_df', None)
                        sample_csv = _format_df_sample(df_scope, max_rows=20, max_cols=10)
                        impact_records = st.session_state.get('last_impact_df_records', [])
                        try:
                            top10 = pd.DataFrame(impact_records).sort_values('é‡è¦æ€§æ’å').head(10)
                            top_impacts_json = top10.to_json(orient='records', force_ascii=False)
                        except Exception:
                            top_impacts_json = "[]"
                        # é€‰ä¸­ç´¢å¼•ä¸å¾—åˆ†
                        sidx = st.session_state.get('last_selected_index')
                        ascore = st.session_state.get('last_selected_anomaly_score')
                        report_lines = st.session_state.get('last_report_lines', [])
                        base_messages = [
                            {"role": "system", "content": build_audit_prompt_header()},
                            {"role": "user", "content": (
                                f"å¼‚å¸¸è®°å½•IDï¼š{sidx}ï¼Œå¼‚å¸¸å¾—åˆ†ï¼š{ascore:.4f}\n\n"
                                f"æ•°æ®æ ·æœ¬(â‰¤20è¡ŒÃ—10åˆ— CSV)ï¼š\n{sample_csv}\n\n"
                                f"å·²ç”Ÿæˆè¦ç‚¹ï¼ˆMarkdownï¼‰ï¼š\n" + "\n".join(report_lines) + "\n\n"
                                f"ç‰¹å¾å½±å“Top10ï¼ˆJSONï¼‰ï¼š\n{top_impacts_json}\n\n"
                                f"é¢å¤–æŒ‡ä»¤ï¼š{custom_instruction}"
                            )}
                        ]
                        if stream_on:
                            placeholder = st.empty()
                            acc = ""
                            for piece in call_deepseek_stream(base_messages, model=model_choice):
                                acc += piece
                                placeholder.markdown(acc)
                            st.session_state['ds_result_text'] = acc
                        else:
                            ai_text = call_deepseek(base_messages, model=model_choice)
                            st.session_state['ds_result_text'] = ai_text
                    if st.session_state.get('ds_result_text'):
                        st.markdown("---")
                        st.markdown("#### DeepSeek ç”Ÿæˆçš„æ–‡å­—ç»“æœ")
                        st.markdown(st.session_state['ds_result_text'])
    # --- 6. AI æ•°æ®åŠ©æ‰‹ï¼ˆDeepSeekï¼‰ ---
    st.markdown("---")
    st.header(" 6. AI æ•°æ®åŠ©æ‰‹ï¼ˆDeepSeekï¼‰")

    if not _HAS_OPENAI:
        st.info("å¦‚éœ€å¯ç”¨ï¼Œè¯·å…ˆå®‰è£… openaiï¼š`pip install openai`ï¼Œå¹¶é…ç½® `DEEPSEEK_API_KEY`ï¼ˆæˆ–ä½¿ç”¨æœ¬æ–‡ä»¶å†…ç½®çš„å¤‡ç”¨Keyï¼‰ã€‚")

    ds_col1, ds_col2 = st.columns([2, 1])
    with ds_col1:
        scope = st.selectbox(
            "é€‰æ‹©æ•°æ®èŒƒå›´",
            ["åŸå§‹æ•°æ®(df)", "é¢„å¤„ç†æ•°æ®(processed_df)", "å¼‚å¸¸è®°å½•(anomalies_df)"],
            index=2
        )
        user_q = st.text_area(
            "å‘ AI æé—® / æŒ‡ä»¤",
            value=("è¯·ç”¨ä¸­æ–‡ç»™å‡ºæ•°æ®è¦ç‚¹ï¼šæ€»ä½“è§„æ¨¡ã€é‡è¦å­—æ®µã€å¼‚å¸¸æ¯”ä¾‹ã€ä¸è¡Œä¸šå¸¸è¯†çš„æ˜¾è‘—åç¦»ï¼Œå¹¶ç»™å‡º3æ¡å¯æ‰§è¡Œçš„å®¡è®¡å¤æ ¸å»ºè®®ã€‚"),
            height=120
        )
        stream_freechat = st.checkbox("å®æ—¶æ˜¾ç¤ºï¼ˆæµå¼ï¼‰", value=True, help="å‹¾é€‰åå°†è¾¹ç”Ÿæˆè¾¹æ˜¾ç¤ºã€‚", key="cb_stream_freechat")
        go = st.button("ğŸ” ç”ŸæˆAIæ–‡å­—ç»“æœ", key="btn_ds_freechat")
    with ds_col2:
        st.markdown("**ä½¿ç”¨æç¤º**ï¼š\n- ä»…å‘é€å‰ 30 è¡ŒÃ—10 åˆ—æ ·æœ¬ä¸ç®€è¦ schemaï¼Œä¿æŠ¤éšç§\n- å¦‚éœ€è‹±æ–‡ï¼Œè¯·åœ¨æŒ‡ä»¤ä¸­è¯´æ˜\n- ç”Ÿæˆå†…å®¹ä»…ä¾›å®¡è®¡è¾…åŠ©")

    if go:
        choose = {
            "åŸå§‹æ•°æ®(df)": st.session_state.get('df'),
            "é¢„å¤„ç†æ•°æ®(processed_df)": st.session_state.get('processed_df'),
            "å¼‚å¸¸è®°å½•(anomalies_df)": st.session_state.get('anomalies_df'),
        }.get(scope)
        hdr = build_audit_prompt_header()
        sample_csv = _format_df_sample(choose, max_rows=30, max_cols=10)
        schema_csv = _schema_summary(choose)
        stats = {}
        try:
            if choose is not None and not choose.empty:
                stats = {
                    "rows": int(len(choose)),
                    "cols": int(len(choose.columns)),
                    "missing_cells": int(choose.isnull().sum().sum()),
                }
        except Exception:
            stats = {}
        messages = [
            {"role": "system", "content": hdr},
            {"role": "user", "content": (
                f"ç”¨æˆ·æŒ‡ä»¤ï¼š{user_q}\n\n"
                f"æ•°æ®èŒƒå›´ï¼š{scope}\n"
                f"ç»Ÿè®¡æ‘˜è¦(JSON)ï¼š{stats}\n\n"
                f"Schema æ¦‚è§ˆ(CSV)ï¼š\n{schema_csv}\n\n"
                f"æ•°æ®æ ·æœ¬(â‰¤30è¡ŒÃ—10åˆ— CSV)ï¼š\n{sample_csv}"
            )}
        ]
        model_name = st.session_state.get('deepseek_model', 'deepseek-chat')
        if stream_freechat:
            placeholder = st.empty()
            acc = ""
            for piece in call_deepseek_stream(messages, model=model_name):
                acc += piece
                placeholder.markdown(acc)
            out = acc
        else:
            out = call_deepseek(messages, model=model_name)
        st.subheader("AI ç»“æœ")
        st.markdown(out)


# å¦‚æœæ²¡æœ‰ä¸Šä¼ æ–‡ä»¶ï¼Œæ˜¾ç¤ºæŒ‡å¼•
else:
    st.info("â¬†ï¸ è¯·ä¸Šä¼ å®¡è®¡æ•°æ®æ–‡ä»¶å¼€å§‹åˆ†æ")


    # ç¤ºä¾‹æ•°æ®ä¸‹è½½
    @st.cache_data
    def generate_sample_data():
        """ç”Ÿæˆç¤ºä¾‹æ•°æ®"""
        np.random.seed(42)
        n_samples = 1000

        # æ­£å¸¸æ•°æ®
        normal_data = {
            'äº¤æ˜“é‡‘é¢': np.random.normal(1000, 300, n_samples),
            'äº¤æ˜“æ—¶é—´é—´éš”': np.random.exponential(5, n_samples),
            'è´¦æˆ·å†å²äº¤æ˜“ç¬”æ•°': np.random.poisson(50, n_samples),
            'äº¤æ˜“ç±»å‹': np.random.choice([0, 1], n_samples, p=[0.3, 0.7]),
            'å•†æˆ·ç±»åˆ«': np.random.choice(['A', 'B', 'C'], n_samples, p=[0.5, 0.3, 0.2])
        }

        # å¼‚å¸¸æ•°æ®
        n_anomalies = 20
        anomaly_data = {
            'äº¤æ˜“é‡‘é¢': np.concatenate([
                np.random.uniform(5000, 10000, n_anomalies // 2),
                np.random.uniform(10, 50, n_anomalies // 2)
            ]),
            'äº¤æ˜“æ—¶é—´é—´éš”': np.random.uniform(0.1, 1, n_anomalies),
            'è´¦æˆ·å†å²äº¤æ˜“ç¬”æ•°': np.random.randint(1, 5, n_anomalies),
            'äº¤æ˜“ç±»å‹': np.random.choice([0, 1], n_anomalies, p=[0.8, 0.2]),
            'å•†æˆ·ç±»åˆ«': np.random.choice(['A', 'D'], n_anomalies, p=[0.3, 0.7])
        }

        # åˆå¹¶æ•°æ®
        df_normal = pd.DataFrame(normal_data)
        df_anomaly = pd.DataFrame(anomaly_data)

        return pd.concat([df_normal, df_anomaly], ignore_index=True)


    sample_df = generate_sample_data()
    csv = sample_df.to_csv(index=False).encode('utf-8')

    st.download_button(
        label="â¬‡ï¸ ä¸‹è½½ç¤ºä¾‹æ•°æ® (sample_audit_data.csv)",
        data=csv,
        file_name='sample_audit_data.csv',
        mime='text/csv',
        help="ç‚¹å‡»ä¸‹è½½ç¤ºä¾‹å®¡è®¡æ•°æ®è¿›è¡Œæµ‹è¯•"
    )